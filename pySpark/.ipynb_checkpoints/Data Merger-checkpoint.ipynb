{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the path to my spark(you may not need these lines): \n",
    "import sys\n",
    "sys.path.append('/home/victor/Downloads/spark-2.3.1-bin-hadoop2.7/python')\n",
    "sys.path.append('/home/victor/Desktop/temp/TILES/src/pyspark_scripts')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from loadCSV import *\n",
    "from MLModels import MLModel\n",
    "from PySparkDFGetWindows import getWindows\n",
    "from computeStats import computeStats\n",
    "from datetime import timedelta, datetime\n",
    "import pyspark\n",
    "import os\n",
    "from shutil import copyfile\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Path to the folder which contains all the subfolders(1,2,3,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AWS Downloading the data.docx', 'TheBigDF', '4_extracted_features', '3_preprocessed_data', 'intermediateData', '1_raw_data', '2_raw_csv']\n",
      "\n",
      "your output should looks like this: 1_raw_data, 2_raw_csv, 3_preprocessed_data, 4_extracted_features\n"
     ]
    }
   ],
   "source": [
    "# !!!!Modify the path!!!!!!\n",
    "path = '/home/victor/Desktop/Data/keck_wave2_gdrive'\n",
    "outputPath = '/home/victor/Desktop/results'\n",
    "if not os.path.isdir(outputPath):\n",
    "    os.mkdir(outputPath)\n",
    "print (os.listdir(path) )\n",
    "print ('\\nyour output should looks like this: 1_raw_data, 2_raw_csv, 3_preprocessed_data, 4_extracted_features')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init Spark\n",
    "sqlContext = initSpark()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start horizontally merging data one by one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "       Merge RealizeD "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loading completed at:  1533161249.5963514\n",
      "Verical Merge completed at:  1533161249.6656828\n",
      "Data load and aggregation complete!!!\n",
      "root\n",
      " |-- Timestamp: timestamp (nullable = true)\n",
      " |-- SecondsOnPhone: double (nullable = true)\n",
      " |-- ID: string (nullable = false)\n",
      " |-- date: date (nullable = true)\n",
      "\n",
      "Saving....Done!!\n"
     ]
    }
   ],
   "source": [
    "# RealizeD is in the fourth folder\n",
    "df = loadAndAggregateDataFromDir(path+'/'+'4_extracted_features', [\"realizd\"])\n",
    "df.printSchema()\n",
    "df.repartition(1).write.mode('overwrite').csv(path+'/intermediateData'+'/realizeD', header=\"true\")\n",
    "del df\n",
    "print ('Saving....Done!!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Merge phone events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loading completed at:  1533161315.0065656\n",
      "Verical Merge completed at:  1533161327.7242188\n",
      "Data load and aggregation complete!!!\n",
      "root\n",
      " |-- Timestamp: timestamp (nullable = true)\n",
      " |-- ssid: string (nullable = true)\n",
      " |-- wifi_mac: string (nullable = true)\n",
      " |-- phone_battery: integer (nullable = true)\n",
      " |-- gps_lat: string (nullable = true)\n",
      " |-- gps_lon: string (nullable = true)\n",
      " |-- ID: string (nullable = false)\n",
      " |-- date: date (nullable = true)\n",
      "\n",
      "Saving....Done!!\n"
     ]
    }
   ],
   "source": [
    "# phone_event is in the fourth folder\n",
    "df = loadAndAggregateDataFromDir(path+'/'+'4_extracted_features', [\"phone_events\"])\n",
    "df.printSchema()\n",
    "df.repartition(1).write.mode('overwrite').csv(path+'/intermediateData'+'/phone_events', header=\"true\")\n",
    "del df\n",
    "print ('Saving....Done!!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Merge app_surveys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loading completed at:  1533161351.3306835\n",
      "Verical Merge completed at:  1533161369.2070286\n",
      "Data load and aggregation complete!!!\n",
      "root\n",
      " |-- Timestamp: string (nullable = true)\n",
      " |-- surveyType: string (nullable = true)\n",
      " |-- durationInSeconds: string (nullable = true)\n",
      " |-- workBeforeSurvey: string (nullable = true)\n",
      " |-- onPhoneBeforeSurvey: string (nullable = true)\n",
      " |-- interactWithPeopleBeforeSurvey: string (nullable = true)\n",
      " |-- sportBeforeSurvey: string (nullable = true)\n",
      " |-- shoppingBeforeSurvey: string (nullable = true)\n",
      " |-- diningBeforeSurvey: string (nullable = true)\n",
      " |-- householingBeforeSurvey: string (nullable = true)\n",
      " |-- familyBeforeSurvey: string (nullable = true)\n",
      " |-- personalActivityBeforeSurvey: string (nullable = true)\n",
      " |-- educationBeforeSurvey: string (nullable = true)\n",
      " |-- transportBeforeSurvey: string (nullable = true)\n",
      " |-- orgActivityBeforeSurvey: string (nullable = true)\n",
      " |-- unKnownBeforeSurvey: string (nullable = true)\n",
      " |-- psycFlexPositiveEmotionasBeforeSurvey: string (nullable = true)\n",
      " |-- psycFlexNegativeEmotionasBeforeSurvey: string (nullable = true)\n",
      " |-- psycFlexScore: string (nullable = true)\n",
      " |-- surveyAtHome: string (nullable = true)\n",
      " |-- surveyAtWork: string (nullable = true)\n",
      " |-- surveyInDoors: string (nullable = true)\n",
      " |-- SurveyInAVehicle: string (nullable = true)\n",
      " |-- surveyAtOtherPlaces: string (nullable = true)\n",
      " |-- psycCapWorkEngagementScore: string (nullable = true)\n",
      " |-- psycCapPSYCAPScore: string (nullable = true)\n",
      " |-- pyscCapInterpersonalSupportSCore: string (nullable = true)\n",
      " |-- psycCapChallengeStressorScore: string (nullable = true)\n",
      " |-- psycCapHindranceStressorScore: string (nullable = true)\n",
      " |-- ID: string (nullable = false)\n",
      " |-- date: date (nullable = true)\n",
      "\n",
      "Saving....Done!!\n"
     ]
    }
   ],
   "source": [
    "# app surveys is in the fourth folder\n",
    "df = loadAndAggregateDataFromDir(path+'/'+'4_extracted_features', [\"tiles_app_survey\"])\n",
    "df.printSchema()\n",
    "df.repartition(1).write.mode('overwrite').csv(path+'/intermediateData'+'/tiles_app_survey', header=\"true\")\n",
    "del df\n",
    "print ('Saving....Done!!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Merge app_analytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loading completed at:  1533161384.2089138\n",
      "Verical Merge completed at:  1533161384.2090068\n",
      "Data load and aggregation complete!!!\n",
      "root\n",
      " |-- uid: string (nullable = true)\n",
      " |-- Activity_Feed_opened: integer (nullable = true)\n",
      " |-- App_started: integer (nullable = true)\n",
      " |-- Contact_opened: integer (nullable = true)\n",
      " |-- FAQ_opened: integer (nullable = true)\n",
      " |-- Facebook_Auth: integer (nullable = true)\n",
      " |-- Fitbit_Auth: integer (nullable = true)\n",
      " |-- Instagram_Auth: integer (nullable = true)\n",
      " |-- Internal_survey_opened: integer (nullable = true)\n",
      " |-- Push_instructions_opened: integer (nullable = true)\n",
      " |-- QR_code_opened: integer (nullable = true)\n",
      " |-- Regular_push_opened: integer (nullable = true)\n",
      "\n",
      "Saving....Done!!\n"
     ]
    }
   ],
   "source": [
    "# app surveys is in the fourth folder\n",
    "df = loadAndAggregateDataFromDir(path+'/'+'4_extracted_features', [\"tiles_app_analytics\"])\n",
    "df.printSchema()\n",
    "df.repartition(1).write.mode('overwrite').csv(path+'/intermediateData'+'/tiles_app_analytics', header=\"true\")\n",
    "del df\n",
    "print ('Saving....Done!!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Merge fitbit "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving....Done!!\n"
     ]
    }
   ],
   "source": [
    "#dfFit = sqlContext.read.csv(dataPathPre + '/fitbit/*Summary.csv', header = \"true\").repartition(6)\n",
    "fitbitPath = path + '/3_preprocessed_data/fitbit/'\n",
    "dfFitList = [] # a dict to hold all the temp dfs; each creted from one file\n",
    "i = 0\n",
    "listDir = sorted(os.listdir(fitbitPath))    \n",
    "for filename in listDir:\n",
    "    # If file is not csv: skip, Else create a df\n",
    "    if not(filename.endswith(\".csv\")) or \"Summary\" not in filename:\n",
    "        continue\n",
    "    dfTemp = sqlContext.read.csv(fitbitPath + filename, header = \"true\")\n",
    "\n",
    "    # If ID is there in the filename, strip it out and add a column to DF for it\n",
    "    if '-' in filename:\n",
    "        x = filename.find('-')\n",
    "        ID = filename[x-8:x+28]\n",
    "        #print(ID)\n",
    "        dfTemp = dfTemp.withColumn(\"ID\", lit(ID))\n",
    "\n",
    "    # If there is Timestamp in df then convert it to date\n",
    "    if \"Timestamp\" in dfTemp.columns:\n",
    "        dfTemp = dfTemp.withColumn('date', to_date(dfTemp.Timestamp))\n",
    "    dfFitList.append(dfTemp)\n",
    "    i = i+1\n",
    "    \n",
    "# Vertical merge all the DF in the list    \n",
    "dfFit = reduce(lambda a,b: a.union(b), dfFitList)\n",
    "dfFit.repartition(1).write.mode('overwrite').csv(path+'/intermediateData'+'/fitbit', header=\"true\")\n",
    "del dfFit\n",
    "print ('Saving....Done!!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Merge omSignal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loading completed at:  1533162477.070927\n",
      "Verical Merge completed at:  1533162500.391785\n",
      "Data load and aggregation complete!!!\n",
      "root\n",
      " |-- Timestamp: timestamp (nullable = true)\n",
      " |-- BreathingDepth: double (nullable = true)\n",
      " |-- BreathingRate: double (nullable = true)\n",
      " |-- Cadence: double (nullable = true)\n",
      " |-- HeartRate: double (nullable = true)\n",
      " |-- Intensity: double (nullable = true)\n",
      " |-- Steps: double (nullable = true)\n",
      " |-- AngleFromVertical_rad: string (nullable = true)\n",
      " |-- AvgBreathingDepth: string (nullable = true)\n",
      " |-- AvgBreathingRate: string (nullable = true)\n",
      " |-- AvgGForce: string (nullable = true)\n",
      " |-- AvgHeartRate: string (nullable = true)\n",
      " |-- AvgXAccel_g: string (nullable = true)\n",
      " |-- AvgYAccel_g: string (nullable = true)\n",
      " |-- AvgZAccel_g: string (nullable = true)\n",
      " |-- Sitting: string (nullable = true)\n",
      " |-- Supine: string (nullable = true)\n",
      " |-- LowGCoverage: string (nullable = true)\n",
      " |-- RMSStdDev_ms: string (nullable = true)\n",
      " |-- RRPeakCoverage: string (nullable = true)\n",
      " |-- SDNN_ms: string (nullable = true)\n",
      " |-- StdDevBreathingDepth: string (nullable = true)\n",
      " |-- StdDevBreathingRate: string (nullable = true)\n",
      " |-- StdDevGForce: string (nullable = true)\n",
      " |-- StdDevXAccel_g: string (nullable = true)\n",
      " |-- StdDevYAccel_g: string (nullable = true)\n",
      " |-- StdDevZAccel_g: string (nullable = true)\n",
      " |-- RR0: double (nullable = true)\n",
      " |-- RR1: string (nullable = true)\n",
      " |-- RR2: string (nullable = true)\n",
      " |-- RR3: string (nullable = true)\n",
      " |-- ID: string (nullable = false)\n",
      " |-- date: date (nullable = true)\n",
      "\n",
      "Saving....Done!!\n"
     ]
    }
   ],
   "source": [
    "df = loadAndAggregateDataFromDir(path+'/'+'3_preprocessed_data', [\"omsignal\"])\n",
    "df.printSchema()\n",
    "df.repartition(1).write.mode('overwrite').csv(path+'/intermediateData'+'/omsignal', header=\"true\")\n",
    "del df\n",
    "print ('Saving....Done!!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Merge Owl_in_one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loading completed at:  1533164160.3325465\n",
      "Verical Merge completed at:  1533164162.0514112\n",
      "Data load and aggregation complete!!!\n",
      "root\n",
      " |-- Timestamp: timestamp (nullable = true)\n",
      " |-- NearestProximity: string (nullable = true)\n",
      " |-- ProximityCertainty: double (nullable = true)\n",
      " |-- AtNursingStation: double (nullable = true)\n",
      " |-- AtPatientRoom: double (nullable = true)\n",
      " |-- AtLounge: double (nullable = true)\n",
      " |-- AtMedicineRoom: double (nullable = true)\n",
      " |-- AtLabRoom: double (nullable = true)\n",
      " |-- AtReceivingRoom: double (nullable = true)\n",
      " |-- ID: string (nullable = false)\n",
      " |-- date: date (nullable = true)\n",
      "\n",
      "Saving....Done!!\n"
     ]
    }
   ],
   "source": [
    "df = loadAndAggregateDataFromDir(path+'/'+'4_extracted_features', [\"owl_in_one\"])\n",
    "df.printSchema()\n",
    "df.repartition(1).write.mode('overwrite').csv(path+'/intermediateData'+'/owl_in_one', header=\"true\")\n",
    "del df\n",
    "print ('Saving....Done!!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Merge minew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = loadAndAggregateDataFromDir(path+'/'+'4_extracted_features', [\"minew\"])\n",
    "df.printSchema()\n",
    "df.repartition(1).write.mode('overwrite').csv(path+'/intermediateData'+'/minew', header=\"true\")\n",
    "del df\n",
    "print ('Saving....Done!!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Merge Ground Truth Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving....Done!!\n"
     ]
    }
   ],
   "source": [
    "dataPath = path + '/' + '3_preprocessed_data' + '/' +'Ground_Truth'\n",
    "idMap = CsvToDF( dataPath + '/'+ 'IDs.csv')\n",
    "igtb = CsvToDF(dataPath +'/' + 'IGTB.csv')\n",
    "mgt = CsvToDF(dataPath + '/' + 'MGT.csv')\n",
    "# merge igtb and mgt\n",
    "merged = igtb.df.join (mgt.df,['timestamp', 'uid'], how = 'full')\n",
    "# replace uid by participant id \n",
    "merged = merged.join(idMap.df, idMap.df.user_id == merged.uid , how = 'inner')\n",
    "merged = merged.drop(\"uid\", 'user_id')\n",
    "merged.repartition(1).write.mode('overwrite').csv(path+'/intermediateData'+'/Ground_Truth', header=\"true\")\n",
    "del merged\n",
    "print ('Saving....Done!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All data is ready for outter join "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tiles_app_survey', 'tiles_app_analytics', 'phone_events', 'omsignal', 'minew', 'realizeD', 'fitbit', 'Ground_Truth', 'owl_in_one']\n"
     ]
    }
   ],
   "source": [
    "intermediate = path+'/intermediateData'\n",
    "folderNames = os.listdir(intermediate)\n",
    "print (folderNames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "stack = []\n",
    "for folder in folderNames:\n",
    "    #get csv file name\n",
    "    fileName = [file  for file in os.listdir(intermediate + '/' + folder) if file.endswith('.csv')][0]\n",
    "    if folder in ['fitbit']:\n",
    "        #copy and paste\n",
    "        copyfile(intermediate+ '/' + folder + '/' + fileName, outputPath+'/'+fileName)\n",
    "    elif folder in ['tiles_app_analytics']:\n",
    "        temp1 = CsvToDF(intermediate+ '/' + folder + '/' + fileName)\n",
    "    elif folder in ['Ground_Truth']:\n",
    "        temp2 = CsvToDF(intermediate+ '/' + folder + '/' + fileName)\n",
    "    else:\n",
    "        #load in data one by one\n",
    "        stack.append(CsvToDF(intermediate+ '/' + folder + '/' + fileName))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the big time-series data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: You have common columns in both dataframes. Merge result might not be accurate.\n",
      "Warning: You have common columns in both dataframes. Merge result might not be accurate.\n",
      "Warning: You have common columns in both dataframes. Merge result might not be accurate.\n",
      "Warning: You have common columns in both dataframes. Merge result might not be accurate.\n",
      "Warning: You have common columns in both dataframes. Merge result might not be accurate.\n",
      "Warning: You have common columns in both dataframes. Merge result might not be accurate.\n"
     ]
    }
   ],
   "source": [
    "# start merging data \n",
    "index = 0\n",
    "myDf = stack[0]\n",
    "while index < len(stack):\n",
    "    myDf = fullOuterJoinOnTimestamp(myDf, stack[index])\n",
    "    index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# dumpt to disk\n",
    "stack[0].repartition(1).write.mode('overwrite').csv(outputPath + '/' + 'others', header=\"true\")\n",
    "print ('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the ID data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "idFile = temp1.df.join (temp2.df,temp1.df.uid == temp2.df.OMuser_id, how = 'full')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "idFile.repartition(1).write.mode('overwrite').csv(outputPath + '/' + 'idFile', header=\"true\")\n",
    "print ('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
